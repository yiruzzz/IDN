{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FBlock(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(FBlock, self).__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Conv2d(3, num_features, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.05),\n",
    "            nn.Conv2d(num_features, num_features, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.05)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.module(x)\n",
    "\n",
    "\n",
    "class DBlock(nn.Module):\n",
    "    def __init__(self, num_features, d, s):\n",
    "        super(DBlock, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.s = s\n",
    "        self.enhancement_top = nn.Sequential(\n",
    "            nn.Conv2d(num_features, num_features - d, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.05),\n",
    "            nn.Conv2d(num_features - d, num_features - 2 * d, kernel_size=3, padding=1, groups=4),\n",
    "            nn.LeakyReLU(0.05),\n",
    "            nn.Conv2d(num_features - 2 * d, num_features, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.05)\n",
    "        )\n",
    "        self.enhancement_bottom = nn.Sequential(\n",
    "            nn.Conv2d(num_features - d, num_features, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.05),\n",
    "            nn.Conv2d(num_features, num_features - d, kernel_size=3, padding=1, groups=4),\n",
    "            nn.LeakyReLU(0.05),\n",
    "            nn.Conv2d(num_features - d, num_features + d, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.05)\n",
    "        )\n",
    "        self.compression = nn.Conv2d(num_features + d, num_features, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.enhancement_top(x)\n",
    "        slice_1 = x[:, :int((self.num_features - self.num_features/self.s)), :, :]\n",
    "        slice_2 = x[:, int((self.num_features - self.num_features/self.s)):, :, :]\n",
    "        x = self.enhancement_bottom(slice_1)\n",
    "        x = x + torch.cat((residual, slice_2), 1)\n",
    "        x = self.compression(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class IDN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(IDN, self).__init__()\n",
    "        self.scale = args.scale\n",
    "        num_features = args.num_features\n",
    "        d = args.d\n",
    "        s = args.s\n",
    "\n",
    "        self.fblock = FBlock(num_features)\n",
    "        self.dblocks = nn.Sequential(*[DBlock(num_features, d, s) for _ in range(4)])\n",
    "        self.deconv = nn.ConvTranspose2d(num_features, 3, kernel_size=17, stride=self.scale, padding=8, output_padding=1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bicubic = F.interpolate(x, scale_factor=self.scale, mode='bicubic', align_corners=False)\n",
    "        x = self.fblock(x)\n",
    "        x = self.dblocks(x)\n",
    "        x = self.deconv(x)\n",
    "        return bicubic + x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
